#!/usr/bin/env python3
"""
Test Pipeline Monitor â€” Diagnostic complet EXO.

Teste en temps rÃ©el :
  1. Niveaux micro (RMS) + calibration bruit ambiant
  2. Seuil VAD adaptatif
  3. Capture d'utterance (durÃ©e, chunks vocaux)
  4. Transcription Whisper (prÃ©cision + latence)
  5. DÃ©tection wake word "EXO"
  6. Extraction commande

Usage:
  python examples/test_pipeline_monitor.py
  python examples/test_pipeline_monitor.py --rounds 10
  python examples/test_pipeline_monitor.py --whisper small
"""

import asyncio
import argparse
import sys
import os
import time
import logging
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

os.environ.setdefault("SUPPRESS_CONFIG_WARNINGS", "1")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("pipeline_monitor")

# â”€â”€â”€ Couleurs terminal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
CYAN = "\033[96m"
BOLD = "\033[1m"
RESET = "\033[0m"

SAMPLE_RATE = 16000
CHUNK_SIZE = 1024


def bar(value: float, max_val: float = 2000, width: int = 40) -> str:
    """Barre visuelle ASCII pour un niveau RMS."""
    ratio = min(value / max_val, 1.0)
    filled = int(ratio * width)
    if ratio < 0.15:
        color = ""
    elif ratio < 0.4:
        color = YELLOW
    else:
        color = GREEN
    return f"{color}{'â–ˆ' * filled}{'â–‘' * (width - filled)}{RESET}"


async def phase_1_mic_levels(stream, duration: float = 5.0):
    """Phase 1 : Affiche les niveaux micro en temps rÃ©el."""
    from src.audio.wake_word import rms_energy

    print(f"\n{BOLD}{'â•' * 60}")
    print(f"  PHASE 1 â€” Niveaux micro ({duration}s)")
    print(f"{'â•' * 60}{RESET}")
    print(f"  Parlez, faites du bruit, restez silencieux...")
    print(f"  Les barres montrent le niveau RMS du micro.\n")

    energies = []
    t0 = time.time()
    while time.time() - t0 < duration:
        try:
            data = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            e = rms_energy(data)
            energies.append(e)
            print(f"\r  RMS: {e:6.0f}  {bar(e)}  ", end="", flush=True)
        except Exception:
            await asyncio.sleep(0.01)
        await asyncio.sleep(0.01)

    print()

    if energies:
        import numpy as np
        arr = np.array(energies)
        print(f"\n  ğŸ“Š Statistiques sur {len(energies)} Ã©chantillons :")
        print(f"     Min : {arr.min():.0f}")
        print(f"     Max : {arr.max():.0f}")
        print(f"     MÃ©diane : {np.median(arr):.0f}")
        print(f"     Moyenne : {arr.mean():.0f}")
        print(f"     Ã‰cart-type : {arr.std():.0f}")
        print(f"     P95 (bruit) : {np.percentile(arr, 95):.0f}")
    return energies


async def phase_2_calibration(stream):
    """Phase 2 : Calibration du bruit ambiant."""
    from src.audio.wake_word import (
        calibrate_noise_floor,
        get_adaptive_threshold,
        DEFAULT_VOICE_THRESHOLD,
        ADAPTIVE_MULTIPLIER,
    )

    print(f"\n{BOLD}{'â•' * 60}")
    print(f"  PHASE 2 â€” Calibration bruit ambiant")
    print(f"{'â•' * 60}{RESET}")
    print(f"  Restez silencieux pendant 2 secondes...\n")

    await asyncio.sleep(0.5)  # Petit dÃ©lai pour que l'utilisateur se prÃ©pare

    noise = calibrate_noise_floor(stream, CHUNK_SIZE)
    threshold = get_adaptive_threshold(DEFAULT_VOICE_THRESHOLD)

    print(f"  ğŸ”‡ Bruit ambiant (mÃ©diane)  : {noise:.0f} RMS")
    print(f"  ğŸ“ Multiplicateur adaptatif  : Ã—{ADAPTIVE_MULTIPLIER}")
    print(f"  ğŸ¯ Seuil adaptatif calculÃ©   : {noise * ADAPTIVE_MULTIPLIER:.0f} RMS")
    print(f"  ğŸ¯ Seuil effectif (bornÃ©)    : {threshold:.0f} RMS")
    print(f"  ğŸ“ Seuil fixe (rÃ©fÃ©rence)    : {DEFAULT_VOICE_THRESHOLD} RMS")

    if threshold < 200:
        print(f"  {YELLOW}âš ï¸  Seuil trÃ¨s bas â€” risque de faux positifs{RESET}")
    elif threshold > 600:
        print(f"  {RED}âš ï¸  Seuil Ã©levÃ© â€” risque de rater des voix douces{RESET}")
    else:
        print(f"  {GREEN}âœ… Seuil dans la plage optimale{RESET}")

    return threshold


async def phase_3_capture_and_stt(stream, whisper_model, rounds: int = 5):
    """Phase 3 : Capture + transcription + wake word."""
    from src.audio.wake_word import (
        capture_utterance,
        contains_wake_word,
        extract_command_after_wake,
        is_hallucination,
        DEFAULT_VOICE_THRESHOLD,
        DEFAULT_SILENCE_CHUNKS,
        DEFAULT_MIN_UTTERANCE_SEC,
    )

    print(f"\n{BOLD}{'â•' * 60}")
    print(f"  PHASE 3 â€” Capture + Whisper ({rounds} tours)")
    print(f"{'â•' * 60}{RESET}")
    print(f"  ModÃ¨le Whisper : {whisper_model}")
    print(f"  Seuil VAD      : {DEFAULT_VOICE_THRESHOLD} (adaptatif activÃ©)")
    print(f"  Silence fin    : {DEFAULT_SILENCE_CHUNKS} chunks (~{DEFAULT_SILENCE_CHUNKS * CHUNK_SIZE / SAMPLE_RATE:.2f}s)")
    print(f"  Min utterance  : {DEFAULT_MIN_UTTERANCE_SEC}s")
    print(f"\n  Dites des phrases â€” essayez Â« Exo, quelle heure est-il ? Â»")
    print(f"  ou Â« Exo, allume la lumiÃ¨re Â»\n")

    # Charger Whisper
    print(f"  â³ Chargement Whisper ({whisper_model})...", end="", flush=True)
    from faster_whisper import WhisperModel
    import numpy as np

    t0 = time.time()
    loop = asyncio.get_running_loop()
    whisper = await loop.run_in_executor(
        None,
        lambda: WhisperModel(whisper_model, device="cpu", compute_type="float32"),
    )
    print(f" OK ({time.time() - t0:.1f}s)")

    def transcribe(audio_bytes: bytes) -> str:
        samples = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
        if len(samples) < 4800:
            return ""
        segments, _ = whisper.transcribe(samples, language="fr", beam_size=1)
        return " ".join(seg.text for seg in segments).strip()

    results = []

    for i in range(rounds):
        print(f"\n  {CYAN}â”€â”€ Tour {i + 1}/{rounds} â”€â”€{RESET}")
        print(f"  ğŸ‘‚ En Ã©coute... (parlez maintenant, timeout 15s)")

        # Capture
        t0_capture = time.time()
        utterance = await capture_utterance(
            stream,
            sample_rate=SAMPLE_RATE,
            chunk_size=CHUNK_SIZE,
            timeout_sec=15.0,
        )
        capture_time = time.time() - t0_capture

        if not utterance:
            print(f"  {YELLOW}â±  Timeout â€” aucune voix dÃ©tectÃ©e{RESET}")
            results.append({"status": "timeout"})
            continue

        duration = len(utterance) / (SAMPLE_RATE * 2)
        print(f"  ğŸ“¦ CapturÃ© : {duration:.2f}s audio ({len(utterance) // 1024}KB) en {capture_time:.2f}s")

        # Transcription
        t0_stt = time.time()
        transcript = await loop.run_in_executor(None, transcribe, utterance)
        stt_time = time.time() - t0_stt

        if not transcript:
            print(f"  {YELLOW}âœ—  Transcription vide{RESET}")
            results.append({"status": "empty", "capture_time": capture_time, "stt_time": stt_time})
            continue

        # Hallucination ?
        if is_hallucination(transcript):
            print(f"  {YELLOW}ğŸ‘» Hallucination filtrÃ©e : Â« {transcript} Â»{RESET}")
            results.append({"status": "hallucination", "text": transcript, "stt_time": stt_time})
            continue

        # RÃ©sultat STT
        print(f"  ğŸ“ Transcrit : Â« {BOLD}{transcript}{RESET} Â» (STT={stt_time:.2f}s)")

        # Wake word ?
        has_wake = contains_wake_word(transcript)
        if has_wake:
            command = extract_command_after_wake(transcript)
            print(f"  {GREEN}âœ¨ WAKE WORD dÃ©tectÃ© !{RESET}")
            if command and len(command.split()) >= 2:
                print(f"  {GREEN}ğŸ’¬ Commande : Â« {command} Â»{RESET}")
            elif command:
                print(f"  {YELLOW}ğŸ”¸ Fragment : Â« {command} Â» (trop court, attente suite){RESET}")
            else:
                print(f"  {YELLOW}ğŸ”¸ Juste Â« Exo Â» â€” pas de commande{RESET}")
        else:
            command = ""
            print(f"  â„¹ï¸  Pas de wake word Â« Exo Â»")

        # Timing
        total = capture_time + stt_time
        speed_indicator = "ğŸŸ¢" if stt_time < 1.0 else ("ğŸŸ¡" if stt_time < 2.0 else "ğŸ”´")
        print(f"  â±  Timing : capture={capture_time:.2f}s + STT={stt_time:.2f}s = {total:.2f}s {speed_indicator}")

        results.append({
            "status": "ok",
            "text": transcript,
            "wake_word": has_wake,
            "command": command,
            "audio_duration": duration,
            "capture_time": capture_time,
            "stt_time": stt_time,
            "total_time": total,
        })

    # RÃ©sumÃ©
    print(f"\n{BOLD}{'â•' * 60}")
    print(f"  RÃ‰SUMÃ‰")
    print(f"{'â•' * 60}{RESET}")

    ok_results = [r for r in results if r.get("status") == "ok"]
    timeouts = sum(1 for r in results if r.get("status") == "timeout")
    hallucinations = sum(1 for r in results if r.get("status") == "hallucination")
    empties = sum(1 for r in results if r.get("status") == "empty")

    print(f"  Tours rÃ©ussis     : {len(ok_results)}/{rounds}")
    print(f"  Timeouts          : {timeouts}")
    print(f"  Hallucinations    : {hallucinations}")
    print(f"  Transcription vide: {empties}")

    if ok_results:
        avg_stt = sum(r["stt_time"] for r in ok_results) / len(ok_results)
        avg_total = sum(r["total_time"] for r in ok_results) / len(ok_results)
        wakes = sum(1 for r in ok_results if r["wake_word"])
        print(f"  Wake words dÃ©tectÃ©s: {wakes}/{len(ok_results)}")
        print(f"  STT moyen         : {avg_stt:.2f}s")
        print(f"  Latence totale moy: {avg_total:.2f}s (capture+STT)")

        print(f"\n  Transcriptions :")
        for r in ok_results:
            wake_tag = f"{GREEN}[EXO]{RESET} " if r["wake_word"] else ""
            print(f"    {wake_tag}Â« {r['text']} Â» ({r['stt_time']:.2f}s)")


async def main():
    parser = argparse.ArgumentParser(description="Test pipeline EXO â€” monitoring micro")
    parser.add_argument("--rounds", type=int, default=5, help="Nombre de tours de capture (default: 5)")
    parser.add_argument("--whisper", type=str, default=os.environ.get("WHISPER_MODEL", "base"),
                        help="ModÃ¨le Whisper (tiny/base/small/medium)")
    parser.add_argument("--skip-levels", action="store_true", help="Sauter la phase niveaux micro")
    parser.add_argument("--device", type=int, default=None, help="Index du micro PyAudio")
    args = parser.parse_args()

    import pyaudio

    print(f"\n{BOLD}{'â•' * 60}")
    print(f"  EXO â€” PIPELINE MONITOR")
    print(f"  Diagnostic complet du pipeline vocal")
    print(f"{'â•' * 60}{RESET}\n")

    # Ouvrir le micro
    pa = pyaudio.PyAudio()

    if args.device is None:
        try:
            info = pa.get_default_input_device_info()
            dev_idx = int(info["index"])
        except Exception:
            dev_idx = 0
    else:
        dev_idx = args.device

    dev_info = pa.get_device_info_by_index(dev_idx)
    print(f"  ğŸ¤ Micro : {dev_info['name']} (index {dev_idx})")
    print(f"  ğŸ“ Sample rate : {SAMPLE_RATE} Hz, chunk : {CHUNK_SIZE}")

    stream = pa.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=SAMPLE_RATE,
        input=True,
        input_device_index=dev_idx,
        frames_per_buffer=CHUNK_SIZE,
    )

    try:
        # Phase 1 : Niveaux micro
        if not args.skip_levels:
            await phase_1_mic_levels(stream, duration=5.0)

        # Phase 2 : Calibration
        await phase_2_calibration(stream)

        # Phase 3 : Capture + STT + wake word
        await phase_3_capture_and_stt(stream, args.whisper, rounds=args.rounds)

    except KeyboardInterrupt:
        print(f"\n\n  âš ï¸  ArrÃªt par l'utilisateur")

    finally:
        stream.stop_stream()
        stream.close()
        pa.terminate()
        print(f"\n  ğŸ”Œ Micro fermÃ©. Test terminÃ©.\n")


if __name__ == "__main__":
    asyncio.run(main())
